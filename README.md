# ETL-pipeline-projects
ğŸ“˜ Telco Customer Churn â€“ ETL & Analysis Project
ğŸ“Œ Overview

This project implements a complete ETL (Extractâ€“Transformâ€“Load) pipeline for the Telco Customer Churn dataset, along with automated data cleaning, Supabase table maintenance, and analytical report generation.

It includes:

ğŸ“¥ Extraction of raw CSV data

ğŸ”„ Transformation & cleaning logic

ğŸ—„ï¸ Loading into Supabase

ğŸ§¹ Automatic removal of empty columns

ğŸ“Š Analysis report generation

ğŸ“ Export of processed results

ğŸ› ï¸ Project Structure
project_root/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ analysis_summary.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ etl_analysis.py
â”‚
â””â”€â”€ README.md

ğŸ“¥ 1. Extract â€” extract.py

The script locates and returns the absolute path of the raw Telco churn CSV file.

âœ” What it does

Identifies the project root directory.

Searches in data/raw/.

Returns full file path.

Throws error if file missing.

ğŸ”„ 2. Transform â€” transform.py

Cleans and prepares the Telco dataset:

Converts data types

Fills missing values

Drops duplicates

Removes invalid rows (e.g., blank TotalCharges)

ğŸ—„ï¸ 3. Load â€” load.py

Loads the cleaned dataset into Supabase using PostgREST API.

Includes:

Insert records

Optional table reset

Optional schema enforcement

ğŸ§¹ 4. Table Cleanup in Supabase
âœ” Delete all rows
DELETE FROM telco_data;

âœ” Remove duplicate rows (no id column needed)
DELETE FROM telco_data t1
USING telco_data t2
WHERE t1.ctid < t2.ctid
AND t1.* = t2.*;

âœ” Remove columns where all values are NULL
DO $$
DECLARE
    col RECORD;
    tbl text := 'telco_data';
    col_name text;
    cnt int;
BEGIN
    FOR col IN
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = tbl
    LOOP
        col_name := col.column_name;
        EXECUTE format('SELECT COUNT(*) FROM %I WHERE %I IS NOT NULL', tbl, col_name) INTO cnt;
        IF cnt = 0 THEN
            EXECUTE format('ALTER TABLE %I DROP COLUMN %I', tbl, col_name);
            RAISE NOTICE 'Dropped column: %', col_name;
        END IF;
    END LOOP;
END$$;

ğŸ“Š 5. Analysis â€” etl_analysis.py

Generates full analytics report from Supabase table.

Metrics Included:

Churn percentage

Avg Monthly Charges per Contract Type

Count of:

New customers

Regular customers

Loyal customers

Champion customers

Internet service distribution

Pivot: Churn vs Tenure Group

Optional visualizations:

Churn rate by Monthly Charge Segment

Histogram: TotalCharges

Contract Type Bar Plot

Output:

Saved in:

data/processed/analysis_summary.csv

â–¶ï¸ Running the Entire Pipeline

To run ETL + analysis:

python src/extract.py
python src/transform.py
python src/load.py
python src/etl_analysis.py


Or create one master script:

python run_pipeline.py

ğŸš€ Future Improvements

Automate Supabase schema enforcement

Add model training (Churn Prediction)

Deploy API + dashboard

Integrate Streamlit dashboards
ğŸš¢ Titanic Dataset â€“ ETL & Analysis Pipeline
ğŸ“Œ Overview

This project implements a complete ETL (Extractâ€“Transformâ€“Load) and Analytics pipeline for the Titanic Passenger Dataset, integrated with a Supabase PostgreSQL database.

It includes:

ğŸ“¥ Extracting Titanic CSV data

ğŸ”„ Cleaning & transforming raw passenger records

ğŸ—„ï¸ Loading cleaned data into Supabase

ğŸ§¹ Removing empty columns from Supabase table

ğŸ§¼ Removing duplicate rows

ğŸ“Š Automated analysis report generation

ğŸ“ Export of summarized metrics

ğŸ› ï¸ Project Structure
project_root/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ titanic.csv
â”‚   â””â”€â”€ processed/
â”‚       â””â”€â”€ analysis_summary.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract.py
â”‚   â”œâ”€â”€ transform.py
â”‚   â”œâ”€â”€ load.py
â”‚   â””â”€â”€ analysis.py
â”‚
â””â”€â”€ README.md

1ï¸âƒ£ Extract â€” extract.py

Reads the Titanic CSV from the data/raw/ folder.

âœ” Features

Automatically locates project root

Validates if file exists

Returns absolute path

2ï¸âƒ£ Transform â€” transform.py

Cleans and preprocesses the Titanic data.

âœ” Operations

Converts datatypes (Age, Fare â†’ numeric)

Handles missing values (Age, Cabin, Embarked)

Drops duplicates

Standardizes column names (optional)

Removes inconsistent or invalid records

3ï¸âƒ£ Load â€” load.py

Loads cleaned Titanic data into Supabase.

âœ” Features

Inserts batch records into your Supabase table

Supports table reset

Optional schema validation

4ï¸âƒ£ Supabase Table Maintenance
ğŸ§¹ Remove ALL duplicate rows

(works even if no id column exists)

DELETE FROM titanic_data t1
USING titanic_data t2
WHERE t1.ctid < t2.ctid
AND t1.* = t2.*;

ğŸ§¹ Remove columns where ALL values are NULL
DO $$
DECLARE
    col RECORD;
    tbl text := 'titanic_data';
    cnt int;
BEGIN
    FOR col IN
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = tbl
    LOOP
        EXECUTE format(
            'SELECT COUNT(*) FROM %I WHERE %I IS NOT NULL',
            tbl, col.column_name
        ) INTO cnt;

        IF cnt = 0 THEN
            EXECUTE format(
                'ALTER TABLE %I DROP COLUMN %I',
                tbl, col.column_name
            );
            RAISE NOTICE 'Dropped column: %', col.column_name;
        END IF;
    END LOOP;
END$$;

5ï¸âƒ£ Analysis â€” analysis.py

Reads the cleaned Titanic data from Supabase and generates analytics.

ğŸ“Š Metrics Generated

Survival rate

Survival % by gender

Survival % by passenger class (Pclass)

Average age of survivors vs non-survivors

Fare statistics

Embarkation distribution

Family size & survival correlation

Pivot table: Survival vs Pclass

OPTIONAL charts:

Age distribution histogram

Fare distribution histogram

Survival rate by gender bar chart

ğŸ“ Output saved to:
data/processed/analysis_summary.csv

â–¶ï¸ Running the Entire Pipeline

Run step-by-step:

python src/extract.py
python src/transform.py
python src/load.py
python src/analysis.py


Or combine into one script:

python run_pipeline.py

ğŸ“¦ Requirements

Python 3.8+

pandas

numpy

supabase-py

matplotlib (optional for plots)

python-dotenv

A sample requirements.txt can be generated if you want it.

ğŸš€ Future Extensions

Train ML survival prediction model

Deploy model using FastAPI

Add Streamlit dashboard

Add automated Supabase triggers for updates
